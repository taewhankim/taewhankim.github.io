---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I am a principal researcher in [Deep Learning Group](https://www.microsoft.com/en-us/research/group/deep-learning-group/) at Microsoft Research, Redmond, directed by [Dr. Jianfeng Gao](http://research.microsoft.com/en-us/um/people/jfgao/). Prior to joining Microsoft at March 2020, I earned my Ph.D. in Computer Science from School of Interactive Computing at [Georgia Tech](https://www.gatech.edu) with thesis "[**Structured Visual Understanding, Generation and Reasoning**](https://repository.gatech.edu/entities/publication/93299669-0326-445a-adbb-b6050cd2eec9)". I was fortunate to be supervised by [Prof. Devi Parikh](https://cc.gatech.edu/~parikh/) and work closely with [Prof. Dhruv Batra](https://www.cc.gatech.edu/~dbatra/).
<!-- My research interests span in computer vision, vision & language and machine learning. More specifically, my primary researches are about structured visual understanding at different levels and how to further leverage them for intelligent interactions with human through language and environment through embodiment. I believe, by integrating fine-grained structured information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agents. -->

My current research is focused on building **generalist multi-modal agent**. Our team are the first few in this line of research, and have produced a series of works, including (a) bridging core vision tasks with language: [UniCL](https://arxiv.org/abs/2204.03610), [RegionCLIP](https://arxiv.org/abs/2112.09106), [GLIP](https://arxiv.org/abs/2112.03857), [KLITE](https://arxiv.org/abs/2204.09222), foundation model [Florence](https://arxiv.org/abs/2111.11432); (b) developing generalist decoder [X-Decoder](https://x-decoder-vl.github.io/), and (c) enabling more promptable, grounding and interactive systems like [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once), [Semantic-SAM](https://github.com/UX-Decoder/Semantic-SAM), [LLaVa](https://github.com/haotian-liu/LLaVA), [SoM Prompting for GPT-4V](https://som-gpt4v.github.io/), and [Phi-3-Vision](https://arxiv.org/pdf/2404.14219). I believe, further pushing forward, we can achieve performant yet interpretable, grounded and robust multi-modality intelligent agents.

**If you are interested in working with me, please feel free to drop me an email through jianwei.yang at microsoft dot com.**

<h2><span style="color:red; font-family:Papyrus">Research News</span></h2>
**\[09/2024\]** Three papers accepted to [NeurIPS 2024](https://neurips.cc/)!<br/>
**\[09/2024\]** [BiomedParse](https://arxiv.org/abs/2405.12971) is accepted by [Nature Methods](https://www.nature.com/nmeth/) and [GigaPath](https://www.nature.com/articles/s41586-024-07441-w) got accepted by [Nature](https://www.nature.com/)! Big congratulations to Health Future team and cheers on the great collaborations!<br/>
**\[08/2024\]** Five papers accepted to [ECCV 2024](https://eccv.ecva.net/), two papers accepted to [CVPR 2024](https://cvpr.thecvf.com/) and one paper accepted to [COLM 2024](https://colmweb.org/)!<br/>
**\[05/2024\]** We announced [Phi-3-Vision](https://arxiv.org/pdf/2404.14219), a 4.2B parameter multimodal model with language and vision capabilities! \[[blog](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/)\] \[[hugging face](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)\]<br/>
**\[10/2023\]** We released [Set-of-Mark (SoM)](https://som-gpt4v.github.io/) prompting to unleash the extraordinary visual grounding power in GPT-4V. Come and try our SoM toolbox!<br/>
<!-- \[10/2023\] We released [BiomedJourney](https://microsoft.github.io/BiomedJourney/), a novel method for counterfactual medical image generation by instruction-learning from multimodal patient journeys! Check it out!<br/> -->
**\[09/2023\]** [Segment Everything Everywhere All at Once (SEEM)](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) has been accepted by NeurIPS 2023!<br/>
**\[09/2023\]** Check out our Survey Paper and Book on [Multimodal Foundation Models: From Specialists to General-Purpose Assistants](https://arxiv.org/pdf/2309.10020.pdf)!<br/>
**\[07/2023\]** We are releasing [Semantic-SAM](https://github.com/UX-Decoder/Semantic-SAM), a universal image segmentation model to enable segment and recognize anything at any desired granularity!<br/>
**\[04/2023\]** We introduce [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) that can Segment Everything Everywhere with Multi-modal prompts all at once!<br/>
**\[12/2022\]** We released [X-Decoder](https://x-decoder-vl.github.io/), a generalist model for decoding pixel-level masks and token-level semantics seamlessly!<br/>
<!-- \[11/2022\] We wrote a [blog post](https://www.microsoft.com/en-us/research/group/deep-learning-group/articles/focalnets-focusing-the-eyes-with-focal-modulation/) to explain in a plain way our FocalNet and the difference from Attention in terms of mechanism, interpretability and performance.<br/> -->
<!-- \[10/2022\] We scaled up our FocalNets to huge size, and achieved new SoTA on COCO object detection! [64.2 on minival](https://paperswithcode.com/sota/object-detection-on-coco-minival) and [64.3 on test-dev](https://paperswithcode.com/sota/object-detection-on-coco)! Check out our [new version](https://arxiv.org/abs/2203.11926) and [code](https://github.com/microsoft/FocalNet)!<br/> -->
  <!-- \[09/2022\] Two papers got accepted by NeurIPS 2022, see you in New Orleans, AGAIN!<br/> -->
  <!-- \[09/2022\] We are organizing [CV in the Wild Workshop and Challange](https://computer-vision-in-the-wild.github.io/eccv-2022/), submit your paper and method!<br/> -->
  <!-- \[06/2022\] We are releasing the [code](https://github.com/microsoft/RegionCLIP) for our CVPR 2022 paper [RegionCLIP](https://arxiv.org/abs/2112.09106), and also a [live demo](https://huggingface.co/spaces/CVPR/regionclip-demo) on huggingface!<br/>
  \[04/2022\] We are releasing our CVPR 2022 paper UniCL [paper](https://arxiv.org/abs/2204.03610) [code](https://github.com/microsoft/UniCL) [demo](https://huggingface.co/spaces/CVPR/unicl-zero-shot-img-recog) - a unified contrastive learning paradigm to learn discriminative and semantic-rich representations from image-label AND image-text data seamlessly!<br/>
\[03/2022\] We are releasing FocalNet [paper](https://arxiv.org/abs/2203.11926) [code](https://github.com/microsoft/FocalNet) - a simple, effective and attention-free architecture for vision!<br/>
  \[03/2022\] Three papers got accepted by CVPR 2022, see you in New Orleans!<br/> -->

<style>
table, td, th, tr {
   border: none!important;
   font-size: 14px;
}
</style>

<h2><span style="font-family:Papyrus">Academic Activities</span></h2>
**\[09/2024\]** Have a great panel discussion about the next generation multimodal models at [Microsoft Research Forum Session 4 on Multimodality](https://researchforum.microsoft.com/).<br/>
**\[07&08/2024\]** Serve as an Area Chair for [NeurIPS 2024](https://neurips.cc/) and [ICLR 2025](https://iclr.cc/Conferences/2025).<br/>
**\[06/2024\]** Gave a tutorial on "<u>A Close Look at Vision in Large Multimodal Models</u>" \[[slides](https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2024/Jianwei_vision_in_LMM.pdf)\] \[[youtube](https://youtu.be/bDVbs-fZGUg)\] at [CVPR 2024 Tutorial on Recent Advances in Vision Foundation Models](https://vlp-tutorial.github.io/).<br/>
**\[06/2024\]** Gave a keynote talk on "<u>Promptable Vision Foundation in the Wild: From Head to Tail</u>" at [CVPR 2024 Worshop on Computer Vision for Materials Science](https://sites.google.com/view/cv4ms-cvpr-2024/home).<br/>
**\[06/2024\]** Organized the [3rd Computer Vision in the Wild (CVinW) Workshop at CVPR 2024](https://computer-vision-in-the-wild.github.io/cvpr-2024/).<br/>
**\[05&06/2024\]** Invited talk on "<u>Towards General-Purpose Multimodal Agent</u>" at University of Washington and Together AI.<br/>
**\[07/2023\]** Panel Discussion on AI Frontier at WAIC and invited talk on "<u>Towards General-Purpose Multimodal Agent</u>" at IDEA.<br/>
**\[06/2023\]** Gave a tutorial on "<u>From Representation to Interface: The Evolution of Foundation for Vision Understanding</u>" \[[slides](https://datarelease.blob.core.windows.net/tutorial/vision_foundation_models_2023/slides/Jianwei_CVPR2023_Tutorial.pdf)\] \[[youtube](https://youtu.be/wIcTyutOlDs)\] at [CVPR 2023 Tutorial on Recent Advances in Vision Foundation Models](https://vlp-tutorial.github.io/2023/index.html).<br/>
**\[03/2023\]** We are announcing the [2nd Computer Vision in the Wild (CVinW) Workshop at CVPR 2023](https://computer-vision-in-the-wild.github.io/cvpr-2023/)!<br/>
**\[12/2022\]** Served as an Area Chair for [ICCV 2023](https://iccv2023.thecvf.com/).<br/>
**\[06/2022\]** Gave a tutorial on "<u>Vision Language Pretraining for Image Classification</u>" \[[slides](https://datarelease.blob.core.windows.net/tutorial/VLP-Tutorial_2022/vlp_for_v_part1.pdf)\] \[[youtube](https://youtu.be/Tq7RWYWN2M0)\] at [CVPR 2022 Tutorial on Recent Advances in Vision-and-Language Pretraining](https://vlp-tutorial.github.io/2022/).<br/>
**\[09/2021\]** Guest lecture on "<u>Learning Visual Curiosity for an Agent through Language and Embodiment</u>" \[[youtube](https://www.youtube.com/watch?v=_mSgCTh0s_I)\] at NeurIPS 2021 IGLU Contest.<br/>
<!-- **\[09/2021\]** Invited talk on "**Towards Generic Vision Transformers for Supervised and Self-supervised Representation Learning**"\[[youtube](https://www.youtube.com/watch?v=fk-6JdRjLPw)\] at EPFL.<br/> -->

<h2><span>Selected Preprints</span></h2>
<table cellspacing="0" cellpadding="0">
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/som_teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V.</h3>
  <b>Jianwei Yang</b>*☨, Hao Zhang*, Feng Li*, Xueyan Zou*, Chunyuan Li, Jianfeng Gao.
  <br>
  <em>arXiv</em>, 2023
  <br>
  <div>
    <a href="https://arxiv.org/abs/2310.11441">[paper]</a>
    <a href="https://github.com/microsoft/SoM">[code]</a>
    <a href="https://som-gpt4v.github.io/">[project]</a>
  </div>  
</td>
</tr>  
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/florence-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Florence: A new foundation model for computer vision.</h3>
  Lu Yuan*, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, <b>Jianwei Yang</b>, Michael Zeng, Luowei Zhou, Pengchuan Zhang. (Team members in alphabetic order)
  <br>
  <em>arXiv</em>, 2021
  <br>
  <div>
    <a href="https://arxiv.org/pdf/2111.11432.pdf">[paper]</a>
    <a href="https://github.com/microsoft/UniCL">[research code]</a>
    <a href="https://www.microsoft.com/en-us/research/blog/azure-ai-milestone-new-foundation-model-florence-v1-0-pushing-vision-and-vision-language-state-of-the-art/">[blog]</a>
  </div>  
</td>
</tr>  
</table>

<h2><span>Selected Publications</span></h2>
<table cellspacing="0" cellpadding="0">
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/semantic-sam.jpg" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Semantic-SAM: Segment and Recognize Anything at Any Granularity.</h3>
  Feng Li*, Hao Zhang*, Peize Sun, Xueyan Zou, Shilong Liu, <b>Jianwei Yang</b>^, Chunyuan Li, Lei Zhang☨, Jianfeng Gao☨.
  <br>
  <em>ECCV</em>, 2024
  <br>
  <div>
    <a href="https://arxiv.org/pdf/2307.04767.pdf">[paper]</a>
    <a href="https://github.com/UX-Decoder/Semantic-SAM">[code]</a>
  </div>  
</td>
</tr>    
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/seem_teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Segment Everything Everywhere all at Once.</h3>
  Xueyan Zou*, <b>Jianwei Yang</b>*^, Hao Zhang*, Feng Li*, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao☨, Yong Jae Lee☨.
  <br>
  <em>NeurIPS</em>, 2023
  <br>
  <div>
    <a href="https://arxiv.org/pdf/2304.06718.pdf">[paper]</a>
    <a href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once">[code]</a>
  </div>  
</td>
</tr>    
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/openseed_framework.jpg" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>A Simple Framework for Open-Vocabulary Segmentation and Detection.</h3>
  Hao Zhang*, Feng Li*, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, <b>Jianwei Yang</b>☨, Lei Zhang☨.
  <br>
  <em>ICCV</em>, 2023
  <br>
  <div>
    <a href="https://x-decoder-vl.github.io/">[project]</a>
    <a href="https://arxiv.org/pdf/2212.11270.pdf">[paper]</a>
    <a href="https://github.com/microsoft/X-Decoder">[code]</a>
    <a href="https://huggingface.co/xdecoder">[huggingface demo]</a>
  </div>  
</td>
</tr>  
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/x-decoder-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Generalized Decoding for Pixel, Image, and Language.</h3>
  Xueyan Zou*, Zi-Yi Dou*, <b>Jianwei Yang</b>*, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee☨, Jianfeng Gao☨.
  <br>
  <em>CVPR</em>, 2023
  <br>
  <div>
    <a href="https://x-decoder-vl.github.io/">[project]</a>
    <a href="https://arxiv.org/pdf/2212.11270.pdf">[paper]</a>
    <a href="https://github.com/microsoft/X-Decoder">[code]</a>
    <a href="https://huggingface.co/xdecoder">[huggingface demo]</a>
  </div>  
</td>
</tr>  
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/focalnet-teaser.png" height="100%" width="100%"  style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Focal Modulation Networks.</h3>
  <b>Jianwei Yang</b>, Chunyuan Li, Xiyang Dai, Lu Yuan and Jianfeng Gao.
  <br>
  <em>NeurIPS</em>, 2022
  <br>
  <div>
    <a href="https://arxiv.org/abs/2203.11926">[paper]</a>
    <a href="https://github.com/microsoft/FocalNet">[code]</a>
    <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/articles/focalnets-focusing-the-eyes-with-focal-modulation/">[blog]</a>
    <a href="https://huggingface.co/spaces/jw2yang/focalnet-modulators">[huggingface demo]</a>
  </div>  
</td>
</tr>
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/klite-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>K-lite: Learning transferable visual models with external knowledge.</h3>
  Sheng Shen*, Chunyuan Li*, Xiaowei Hu, Yujia Xie, <b>Jianwei Yang</b>, Pengchuan Zhang, Anna Rohrbach, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, Kurt Keutzer, Trevor Darrell, Jianfeng Gao.
  <br>
  <em>NeurIPS</em>, 2022. <span style="color:red">Oral</span>
  <br>
  <div>
    <a href="https://arxiv.org/pdf/2204.09222.pdf">[paper]</a>
    <a href="https://github.com/microsoft/klite">[code]</a>
  </div>  
</td>
</tr>
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/glip-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Grounded language-image pre-training.</h3>
  Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, <b>Jianwei Yang</b>, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao.
  <br>
  <em>CVPR</em>, 2022. <span style="color:red">Best Paper Final list</span>
  <br>
  <div>
    <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf">[paper]</a>
    <a href="https://github.com/microsoft/GLIP">[code]</a>
    <a href="https://huggingface.co/spaces/haotiz/glip-zeroshot-demo">[huggingface demo]</a>
  </div>  
</td>
</tr>
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/regionclip-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Regionclip: Region-based language-image pretraining</h3>
  Yiwu Zhong, <b>Jianwei Yang</b>, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, Jianfeng Gao.
  <br>
  <em>CVPR</em>, 2022
  <br>
  <div>
    <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.pdf">[paper]</a>
    <a href="https://github.com/microsoft/RegionCLIP">[code]</a>
    <a href="https://huggingface.co/spaces/CVPR/regionclip-demo">[huggingface demo]</a>
  </div>  
</td>
</tr>
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/unicl-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Unified contrastive learning in image-text-label space.</h3>
  <b>Jianwei Yang</b>*, Chunyuan Li*, Pengchuan Zhang*, Bin Xiao*, Ce Liu, Lu Yuan, Jianfeng Gao.
  <br>
  <em>CVPR</em>, 2022
  <br>
  <div>
    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf">[paper]</a>
    <a href="https://github.com/microsoft/UniCL">[code]</a>
    <a href="https://huggingface.co/spaces/CVPR/unicl-zero-shot-img-recog">[huggingface demo]</a>
  </div>  
</td>
</tr>
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/esvit-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Efficient self-supervised vision transformers for representation learning.</h3>
  Chunyuan Li, <b>Jianwei Yang</b>, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, Jianfeng Gao.
  <br>
  <em>ICLR</em>, 2022
  <br>
  <div>
    <a href="https://arxiv.org/pdf/2106.09785.pdf">[paper]</a>
    <a href="https://github.com/microsoft/esvit">[code]</a>
  </div>  
</td>
</tr>
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/focal-transformer-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Focal attention for long-range interactions in vision transformers.</h3>
  <b>Jianwei Yang</b>, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao.
  <br>
  <em>NeurIPS</em>, 2021, <span style="color:red">Spotlight</span>.
  <br>
  <div>
    <a href="https://proceedings.neurips.cc/paper/2021/file/fc1a36821b02abbd2503fd949bfc9131-Paper.pdf">[paper]</a>
    <a href="https://github.com/microsoft/Focal-Transformer">[code]</a>
    <a href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjzk6Wm8NHyAhVCqlsKHYepD9wQtwJ6BAgDEAM&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYH319yyeoVw&usg=AOvVaw27s7EE-txctmc6_BwKnnfE">[video]</a>
  </div>  
</td>
</tr>
<tr>
<td style="padding:0px;width:30%;vertical-align:middle">
  <img src="../images/taco-teaser.png" height="100%" width="100%" style="border-style: none">
</td>
<td style="padding:20px;width:70%;vertical-align:middle">  
  <h3>Taco: Token-aware cascade contrastive learning for video-text alignment.</h3>
  <b>Jianwei Yang</b>, Yonatan Bisk, Jianfeng Gao.
  <br>
  <em>ICCV</em>, 2021
  <br>
  <div>
    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_TACo_Token-Aware_Cascade_Contrastive_Learning_for_Video-Text_Alignment_ICCV_2021_paper.pdf">[paper]</a>
  </div>  
</td>
</tr>
</table>
